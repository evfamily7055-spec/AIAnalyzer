import streamlit as st
import pandas as pd
import io
import json
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions
import traceback

# ã‚°ãƒ©ãƒ•æç”»ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import plotly.express as px
import plotly.graph_objects as go

# æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ï¼ˆå½¢æ…‹ç´ è§£æï¼‰ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from janome.tokenizer import Tokenizer
    from janome.tokenfilter import POSKeepFilter, TokenCountFilter
    from janome.analyzer import Analyzer
    JANOME_AVAILABLE = True
except ImportError:
    JANOME_AVAILABLE = False

# çµ±è¨ˆçš„ä»®èª¬æ¤œå®šãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from scipy import stats
    import statsmodels.api as sm
    STATS_LIBS_AVAILABLE = True
except ImportError:
    STATS_LIBS_AVAILABLE = False

# --- å®šæ•° (Constants) ---
MAX_UNIQUE_VALUES_FOR_SCHEMA = 20

# --- ãƒšãƒ¼ã‚¸è¨­å®š (Page Config) ---
st.set_page_config(layout="wide")
st.title("AIãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆ (ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯å®Ÿè¡Œ) ğŸš€")
st.info("é›†è¨ˆãƒ»å¯è¦–åŒ–ãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ãƒ»çµ±è¨ˆæ¤œå®šãƒ»è«–æ–‡ç”¨è§£èª¬ã®ç”Ÿæˆã¾ã§ã€AIãŒãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã§å®Ÿè¡Œã—ã¾ã™ã€‚")

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ– (Initialize Session State) ---
if 'df' not in st.session_state:
    st.session_state.df = None 
if 'schema_dict' not in st.session_state:
    st.session_state.schema_dict = None 
if 'generated_code' not in st.session_state:
    st.session_state.generated_code = "" 
if 'exec_output' not in st.session_state:
    st.session_state.exec_output = None 
if 'analysis_explanation' not in st.session_state:
    st.session_state.analysis_explanation = "" 
if 'statistical_interpretation' not in st.session_state:
    st.session_state.statistical_interpretation = ""
if 'last_uploaded_filename' not in st.session_state:
    st.session_state.last_uploaded_filename = None

# --- Gemini API å‘¼ã³å‡ºã—é–¢æ•° ---
@st.cache_data(ttl=600) 
def generate_code_and_explanation(schema_json: str, user_prompt: str, api_key: str):
    """
    æ‹¡å¼µã‚¹ã‚­ãƒ¼ãƒã¨æŒ‡ç¤ºã‚’Geminiã«é€ä¿¡ã—ã€
    ã€Œã‚³ãƒ¼ãƒ‰ã€ã€Œåˆ†æèª¬æ˜ã€ã€Œçµ±è¨ˆçš„è§£é‡ˆã€ã‚’å«ã‚€JSONã‚’ç”Ÿæˆã™ã‚‹ã€‚
    """
    try:
        genai.configure(api_key=api_key)
    except Exception as e:
        st.error(f"APIã‚­ãƒ¼ã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

    system_prompt = (
        "ã‚ãªãŸã¯ã€Pandas, Plotly (px), Janome (NLP), Scipy (stats), Statsmodels (sm) ã‚’å°‚é–€ã¨ã™ã‚‹ä¸–ç•Œã‚¯ãƒ©ã‚¹ã®Pythonãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚"
        "ã‚ãªãŸã®ä»•äº‹ã¯ã€æ¸¡ã•ã‚ŒãŸã€Œãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒã€ã¨ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ›–æ˜§ãªæŒ‡ç¤ºã€ã‹ã‚‰ã€ã€Œå®Ÿè¡Œã‚³ãƒ¼ãƒ‰ã€ã€Œåˆ†æå†…å®¹ã®æ—¥æœ¬èªèª¬æ˜ã€ã€Œçµ±è¨ˆçš„è§£é‡ˆã€ã®3ã¤ã‚’ *JSONå½¢å¼* ã§ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚"
        
        "## ãƒ«ãƒ¼ãƒ«:"
        "1. å‡ºåŠ›ã¯ *å¿…ãš* ä»¥ä¸‹ã®JSONå½¢å¼ã® *æ–‡å­—åˆ—ã®ã¿* ã¨ã—ã¦ãã ã•ã„ã€‚èª¬æ˜ã‚„ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ï¼ˆ```json ãªã©ï¼‰ã¯çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚"
        "   {\n"
        "     \"code_to_execute\": \"... (ã“ã“ã«Pythonã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’è¨˜è¿°) ...\",\n"
        "     \"analysis_explanation\": \"... (ã“ã“ã«åˆ†æå†…å®¹ã®æ—¥æœ¬èªèª¬æ˜ã‚’è¨˜è¿°) ...\",\n"
        "     \"statistical_interpretation\": \"... (ã“ã“ã«çµ±è¨ˆæ¤œå®šã®çµæœã®è§£é‡ˆã‚’è¨˜è¿°) ...\"\n"
        "   }\n"
        
        "2. `code_to_execute` ã®ãƒ«ãƒ¼ãƒ«:"
        "   - å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¯ *å¸¸ã«* `df` ã¨ã„ã†åå‰ã§ã™ã€‚"
        "   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã‹ã‚‰ã€é›†è¨ˆ(Pandas), ã‚°ãƒ©ãƒ•(Plotly as px), NLP(Janome), çµ±è¨ˆæ¤œå®š(scipy.stats as stats, statsmodels.api as sm) ã®ã©ã‚ŒãŒæœ€é©ã‹ *æ¨è«–* ã—ã¦ãã ã•ã„ã€‚"
        "   - (STABILITY) ã‚°ãƒ©ãƒ•æç”»æ™‚ (px.pie, px.bar ãªã©) ã€`template` ã®ã‚ˆã†ãªå¤–è¦³ã«é–¢ã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ *çµ¶å¯¾* ã«æŒ‡å®šã—ãªã„ã§ãã ã•ã„ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚"
        "   - ã‚³ãƒ¼ãƒ‰ã® *æœ€çµ‚è¡Œ* ã¯ã€çµæœï¼ˆDataFrame, Series, Plotly Figure, ã¾ãŸã¯æ¤œå®šçµæœã®æ–‡å­—åˆ—/DataFrameï¼‰ã‚’ `output` ã¨ã„ã†å˜ä¸€ã®å¤‰æ•°ã« *å¿…ãš* ä»£å…¥ã—ã¦ãã ã•ã„ã€‚"
        "   - `print()` ã‚„ `fig.show()` æ–‡ã¯ *çµ¶å¯¾ã«* ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚"
        
        "3. `analysis_explanation` ã®ãƒ«ãƒ¼ãƒ«:"
        "   - `code_to_execute` ã§å®Ÿè¡Œã™ã‚‹åˆ†æãŒ *ä½•ã‚’ã—ã¦ã„ã‚‹ã‹* ã‚’ã€å­¦è¡“è«–æ–‡ã®ã€Œæ–¹æ³•ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ä½¿ãˆã‚‹ã€å®¢è¦³çš„ã‹ã¤ç°¡æ½”ãªæ—¥æœ¬èªã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        "   - ï¼ˆä¾‹: ã€Œ'Gender' åˆ—ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã‚­ãƒ¼ã¨ã—ã€'Age' åˆ—ã®å¹³å‡å€¤ã‚’ç®—å‡ºã—ãŸã€‚ã€ï¼‰"

        "4. `statistical_interpretation` ã®ãƒ«ãƒ¼ãƒ«:"
        "   - *çµ±è¨ˆæ¤œå®šã‚’å®Ÿè¡Œã—ãŸå ´åˆã®ã¿*ã€ãã®çµæœï¼ˆpå€¤ã€çµ±è¨ˆé‡ãªã©ï¼‰ã‚’è«–æ–‡ã®ã€Œçµæœã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ä½¿ãˆã‚‹ã‚ˆã†ã«æ—¥æœ¬èªã§è§£é‡ˆã—ã¦ãã ã•ã„ã€‚"
        "   - ï¼ˆä¾‹: ã€Œtæ¤œå®šã®çµæœã€på€¤ã¯0.03ã§ã‚ã‚Šã€5%æ°´æº–ã§æœ‰æ„ãªå·®ãŒèªã‚ã‚‰ã‚ŒãŸã€‚ã€ï¼‰"
        "   - *çµ±è¨ˆæ¤œå®šã§ãªã„å ´åˆï¼ˆå˜ç´”é›†è¨ˆã‚„ã‚°ãƒ©ãƒ•æç”»ï¼‰ã¯ã€ã“ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ç©ºæ–‡å­—åˆ— \"\" ã¨ã—ã¦ãã ã•ã„ã€‚*"

        "5. çµ±è¨ˆæ¤œå®šã®æŒ‡ç¤ºï¼ˆä¾‹: ã€Œå·®ãŒã‚ã‚‹ã‹æ¤œå®šã€ã€Œé–¢é€£ã‚’åˆ†æã€ã€Œç›¸é–¢ã‚’èª¿ã¹ã¦ã€ï¼‰ã®å ´åˆ:"
        "   - ã‚¹ã‚­ãƒ¼ãƒï¼ˆãƒ‡ãƒ¼ã‚¿å‹ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ã®æ•°ï¼‰ã«åŸºã¥ãã€æœ€é©ãªæ¤œå®šæ‰‹æ³•ã‚’ *è‡ªå‹•ã§é¸æŠ* ã—ã¦ãã ã•ã„ã€‚"
        "   - (A) 2ã¤ã®æ•°å€¤å¤‰æ•°ã®é–¢ä¿‚æ€§ -> ç›¸é–¢åˆ†æ (`stats.pearsonr`)ã€‚çµæœã¯ `r, p = stats.pearsonr(...)` ã¨ã—ã€`output = f'ç›¸é–¢ä¿‚æ•°(r): {r:.4f}, på€¤: {p:.4g}'` ã®ã‚ˆã†ã«æ–‡å­—åˆ—ã§è¿”ã—ã¦ãã ã•ã„ã€‚"
        "   - (B) ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°(2ç¾¤) vs æ•°å€¤å¤‰æ•° -> ç‹¬ç«‹2ç¾¤ã®tæ¤œå®š (`stats.ttest_ind`)ã€‚çµæœã¯ `stat, p = stats.ttest_ind(...)` ã¨ã—ã€`output = f'tå€¤: {stat:.4f}, på€¤: {p:.4g}'` ã§è¿”ã—ã¦ãã ã•ã„ã€‚"
        "   - (C) 2ã¤ã®ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®é–¢ä¿‚æ€§ -> ã‚«ã‚¤äºŒä¹—æ¤œå®š (`stats.chi2_contingency`)ã€‚`pd.crosstab` ã§ã‚¯ãƒ­ã‚¹è¡¨ã‚’ä½œæˆã—ã€`chi2, p, dof, ex = stats.chi2_contingency(crosstab)` ã‚’å®Ÿè¡Œã€‚`output = f'ã‚«ã‚¤äºŒä¹—å€¤: {chi2:.4f}, på€¤: {p:.4g}, è‡ªç”±åº¦: {dof}'` ã§è¿”ã—ã¦ãã ã•ã„ã€‚"
        "   - (D) 1ã¤ã®æ•°å€¤å¤‰æ•° (X) ã‹ã‚‰ 1ã¤ã®æ•°å€¤å¤‰æ•° (Y) ã‚’äºˆæ¸¬ -> å˜å›å¸°åˆ†æ (`sm.OLS`)ã€‚`X = sm.add_constant(df['X'])`, `model = sm.OLS(df['Y'], X).fit()`, `output = model.summary().as_text()` ã§ *ã‚µãƒãƒªãƒ¼å…¨ä½“ã‚’æ–‡å­—åˆ—ã¨ã—ã¦* è¿”ã—ã¦ãã ã•ã„ã€‚"
        
        "6. (NLP) æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ã®æŒ‡ç¤ºï¼ˆä¾‹: ã€Œè‡ªç”±å›ç­”ã‚’åˆ†æã€ã€Œå˜èªé »åº¦ã€ï¼‰ã®å ´åˆ:"
        "   - `janome.tokenizer.Tokenizer` ã‚’ä½¿ç”¨ã€‚åˆ†æå¯¾è±¡ã¯ *åè©*, *å‹•è©*, *å½¢å®¹è©* ã® *åŸå½¢* ã¨ã—ã¦ãã ã•ã„ã€‚"
        "   - `stop_words` (ä¾‹: 'ã™ã‚‹', 'ã‚ã‚‹', 'ãªã„', 'ã“ã¨', 'ã‚‚ã®') ã‚’å®šç¾©ã—ã€é™¤å¤–ã€‚"
        "   - å˜èªé »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã€ä¸Šä½50ä»¶ã‚’ `pd.DataFrame(..., columns=['word', 'count'])` ã«æ ¼ç´ã€‚"
        "   - æœ€å¾Œã« `px.treemap` ã‚’ä½¿ç”¨ã—ã€`path=[px.Constant('all'), 'word']`, `values='count'` ã§çµæœã‚’å¯è¦–åŒ–ã—ã€ãã‚Œã‚’ `output` ã«ä»£å…¥ã€‚"
    )

    model = genai.GenerativeModel(
        model_name="gemini-2.5-flash-preview-09-2025",
        system_instruction=system_prompt,
        generation_config={"response_mime_type": "application/json"}
    )
    
    full_prompt = (
        f"## ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒ (JSONå½¢å¼):\n{schema_json}\n\n"
        f"## ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤º:\n{user_prompt}"
    )

    try:
        response = model.generate_content(full_prompt)
        response_json = response.text.strip()
        response_data = json.loads(response_json)
        return response_data
    except json.JSONDecodeError as e:
        st.error(f"AIãŒJSONå½¢å¼ã§ãªã„å¿œç­”ã‚’è¿”ã—ã¾ã—ãŸã€‚AIã®å¿œç­”: {response.text}\nã‚¨ãƒ©ãƒ¼: {e}")
        return None
    except google_exceptions.InvalidArgument as e:
        st.error(f"APIã‚­ãƒ¼ãŒç„¡åŠ¹ã€ã¾ãŸã¯è¨­å®šãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“: {e}")
        return None
    except Exception as e:
        st.error(f"Gemini API å‘¼ã³å‡ºã—ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n{traceback.format_exc()}")
        return None

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ (APIã‚­ãƒ¼å…¥åŠ›) ---
with st.sidebar:
    st.header("è¨­å®š")
    api_key = st.text_input("Gemini API Key", type="password", help="Gemini APIã‚­ãƒ¼ã‚’ã“ã“ã«å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    st.markdown("---")
    st.info("ã“ã®ã‚¢ãƒ—ãƒªã¯å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’AIã«é€ä¿¡ã—ã¾ã›ã‚“ã€‚AIã«ã¯åˆ—åã¨ã‚«ãƒ†ã‚´ãƒªã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ï¼ˆ20ç¨®é¡ä»¥ä¸‹ï¼‰ã®ã¿ãŒé€ä¿¡ã•ã‚Œã¾ã™ã€‚")
    if not JANOME_AVAILABLE:
        st.error("Janomeãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ã€‚\n`pip install janome` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    if not STATS_LIBS_AVAILABLE:
        st.error("Scipyã¾ãŸã¯StatsmodelsãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚çµ±è¨ˆæ¤œå®šæ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ã€‚\n`pip install scipy statsmodels` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# --- 1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ ---
uploaded_file = st.file_uploader("Excelãƒ•ã‚¡ã‚¤ãƒ« (.xlsx) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["xlsx"])

# (BUG FIX) 
# uploaded_file ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒNoneã§ãªãã€
# ã‹ã¤ã€Œãƒ•ã‚¡ã‚¤ãƒ«åãŒå‰å›ã¨ç•°ãªã‚‹ã€å ´åˆã«ã®ã¿ã€DFã®èª­ã¿è¾¼ã¿ã¨çŠ¶æ…‹ã®ãƒªã‚»ãƒƒãƒˆã‚’å®Ÿè¡Œã™ã‚‹
if uploaded_file is not None:
    if uploaded_file.name != st.session_state.last_uploaded_filename:
        try:
            st.info(f"'{uploaded_file.name}' ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
            bytes_data = uploaded_file.getvalue()
            df = pd.read_excel(io.BytesIO(bytes_data))
            
            st.session_state.df = df 
            st.session_state.last_uploaded_filename = uploaded_file.name # (FIX) ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨˜æ†¶
            
            # æ‹¡å¼µã‚¹ã‚­ãƒ¼ãƒã®ç”Ÿæˆ
            schema = {}
            for col in df.columns:
                dtype = str(df[col].dtype)
                schema[col] = {"dtype": dtype}
                
                if dtype == 'object' and df[col].nunique() <= MAX_UNIQUE_VALUES_FOR_SCHEMA:
                    unique_vals = df[col].dropna().unique().tolist()
                    schema[col]["unique_values"] = unique_vals
                elif pd.api.types.is_numeric_dtype(df[col]):
                     try:
                         schema[col]["mean"] = float(df[col].mean())
                         schema[col]["min"] = float(df[col].min())
                         schema[col]["max"] = float(df[col].max())
                     except Exception:
                         pass 
                     
            st.session_state.schema_dict = schema
            st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ (è¡Œ: {len(df)}, åˆ—: {len(df.columns)})")
            
            # (FIX) çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
            st.session_state.generated_code = ""
            st.session_state.exec_output = None 
            st.session_state.analysis_explanation = ""
            st.session_state.statistical_interpretation = ""

        except Exception as e:
            st.error(f"Excelãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            st.session_state.df = None
            st.session_state.last_uploaded_filename = None

# --- 2. ãƒ¡ã‚¤ãƒ³ã®ä½œæ¥­é ˜åŸŸ (å·¦å³åˆ†å‰²) ---
if st.session_state.df is not None:
    st.markdown("---")
    
    col1, col2 = st.columns(2)

    # --- å·¦ã‚«ãƒ©ãƒ  (col1): AIã¸ã®æŒ‡ç¤ºã¨å®Ÿè¡Œï¼ˆä½œæ¥­é ˜åŸŸï¼‰ ---
    with col1:
        st.header("Step 1: AIã¸ã®åˆ†ææŒ‡ç¤º")
        st.write("å³å´ã§ãƒ‡ãƒ¼ã‚¿ï¼ˆåˆ—åï¼‰ã‚’ç¢ºèªã—ãªãŒã‚‰ã€å®Ÿè¡Œã—ãŸã„å†…å®¹ã‚’æ—¥æœ¬èªã§æŒ‡ç¤ºã—ã¦ãã ã•ã„ã€‚")
        
        user_prompt = st.text_area(
            "æŒ‡ç¤ºå…¥åŠ›æ¬„:",
            placeholder=(
                "ï¼ˆé›†è¨ˆä¾‹ï¼‰: ã€Œç”·å¥³åˆ¥ã®å¹´é½¢ã®å¹³å‡å€¤ã€\n"
                "ï¼ˆå¯è¦–åŒ–ä¾‹ï¼‰: ã€Œ'å¹´é½¢' ã¨ 'çµ¦ä¸' ã®æ•£å¸ƒå›³ã‚’è¡¨ç¤ºã€\n"
                "ï¼ˆNLPä¾‹ï¼‰: ã€Œ'è‡ªç”±å›ç­”' åˆ—ã®å˜èªé »åº¦ã‚’å¯è¦–åŒ–ã€\n"
                "ï¼ˆæ¤œå®šä¾‹ï¼‰: ã€Œ'ä»‹å…¥ç¾¤' ã¨ 'å¯¾ç…§ç¾¤' ã§ 'ã‚¹ã‚³ã‚¢' ã«å·®ãŒã‚ã‚‹ã‹æ¤œå®šã€\n"
                "ï¼ˆæ¤œå®šä¾‹ï¼‰: ã€Œ'å¹´é½¢' ã¨ 'çµ¦ä¸' ã®ç›¸é–¢ã‚’åˆ†æã—ã¦ã€"
            ),
            height=150
        )

        # (UX CHANGE) ãƒœã‚¿ãƒ³ã‚’ã€Œåˆ†æã‚’å®Ÿè¡Œã€ã«å¤‰æ›´
        if st.button("ğŸ¤– åˆ†æã‚’å®Ÿè¡Œ", type="primary"):
            if not api_key:
                st.error("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰Gemini APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            elif not user_prompt:
                st.warning("æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            
            # ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒã‚§ãƒƒã‚¯
            elif ("ãƒ†ã‚­ã‚¹ãƒˆ" in user_prompt or "å˜èª" in user_prompt or "NLP" in user_prompt) and not JANOME_AVAILABLE:
                 st.error("ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ã«ã¯Janomeãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                 st.stop()
            elif ("æ¤œå®š" in user_prompt or "åˆ†æ" in user_prompt or "å·®" in user_prompt or "é–¢é€£" in user_prompt or "ç›¸é–¢" in user_prompt) and not STATS_LIBS_AVAILABLE:
                 st.error("çµ±è¨ˆæ¤œå®šã«ã¯Scipyã¨StatsmodelsãŒå¿…è¦ã§ã™ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                 st.stop()
            
            # --- (NEW) ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã§ã€Œç”Ÿæˆã€ã¨ã€Œå®Ÿè¡Œã€ã‚’ä¸¡æ–¹è¡Œã† ---
            else:
                with st.spinner("AIãŒã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã€ã‚µãƒ¼ãƒãƒ¼ä¸Šã§å®Ÿè¡Œä¸­ã§ã™..."):
                    # 1. AIã‚³ãƒ¼ãƒ‰ç”Ÿæˆ
                    schema_json = json.dumps(st.session_state.schema_dict, indent=2, ensure_ascii=False)
                    response_data = generate_code_and_explanation(schema_json, user_prompt, api_key)
                    
                    if not response_data or "code_to_execute" not in response_data:
                        st.error("AIã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                        st.stop()

                    st.session_state.generated_code = response_data.get("code_to_execute", "")
                    st.session_state.analysis_explanation = response_data.get("analysis_explanation", "(èª¬æ˜ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ)")
                    st.session_state.statistical_interpretation = response_data.get("statistical_interpretation", "")
                    
                    if not st.session_state.generated_code:
                        st.error("AIã¯å¿œç­”ã—ã¾ã—ãŸãŒã€å®Ÿè¡Œå¯èƒ½ãªã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚")
                        st.stop()

                    # 2. ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ
                    try:
                        global_vars = {"pd": pd, "px": px, "go": go}
                        if JANOME_AVAILABLE:
                            global_vars["Tokenizer"] = Tokenizer
                            global_vars["Analyzer"] = Analyzer
                            global_vars["POSKeepFilter"] = POSKeepFilter
                            global_vars["TokenCountFilter"] = TokenCountFilter
                        if STATS_LIBS_AVAILABLE:
                            global_vars["stats"] = stats
                            global_vars["sm"] = sm
                            
                        local_vars = {"df": st.session_state.df.copy()} 
                        
                        exec(st.session_state.generated_code, global_vars, local_vars)
                        
                        output = local_vars.get("output", None)
                        
                        if output is not None:
                            st.session_state.exec_output = output
                            st.success("åˆ†æãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸã€‚Step 2ã§çµæœã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                        else:
                            st.session_state.exec_output = None
                            st.error("ã‚³ãƒ¼ãƒ‰ã¯å®Ÿè¡Œã•ã‚Œã¾ã—ãŸãŒã€'output' å¤‰æ•°ã«çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                            
                    except Exception as e:
                        st.session_state.exec_output = None
                        st.error(f"ã‚³ãƒ¼ãƒ‰ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\n{traceback.format_exc()}")

        st.markdown("---")
        
        # (UX CHANGE) Step 2 ã‚’ã€Œå®Ÿè¡Œçµæœã€ã«å¤‰æ›´
        st.header("Step 2: å®Ÿè¡Œçµæœã¨åˆ†æã®è§£èª¬")
        
        if st.session_state.analysis_explanation:
            st.subheader("åˆ†æå†…å®¹ã®è§£èª¬ï¼ˆè«–æ–‡ã®ã€Œæ–¹æ³•ã€ç”¨ï¼‰")
            st.success(f"ğŸ“„ {st.session_state.analysis_explanation}")
        
        if st.session_state.statistical_interpretation:
            st.subheader("çµ±è¨ˆçš„è§£é‡ˆï¼ˆè«–æ–‡ã®ã€Œçµæœã€ç”¨ï¼‰")
            st.info(f"ğŸ“ˆ {st.session_state.statistical_interpretation}")
        
        if st.session_state.exec_output is not None:
            
            output = st.session_state.exec_output
            
            # 1. çµæœãŒãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ« (DataFrame or Series) ã®å ´åˆ
            if isinstance(output, (pd.DataFrame, pd.Series)):
                st.subheader("é›†è¨ˆãƒ»åˆ†æçµæœ (ãƒ†ãƒ¼ãƒ–ãƒ«)")
                st.dataframe(output, use_container_width=True)
                
                @st.cache_data
                def convert_df_to_csv(df_to_convert):
                    if not isinstance(df_to_convert, pd.DataFrame):
                        df_to_convert = df_to_convert.to_frame()
                    return df_to_convert.to_csv(index=True).encode('utf-8-sig')
                
                try:
                    csv_data = convert_df_to_csv(output)
                    st.download_button(label="çµæœã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_data, file_name="analysis_result.csv", mime="text/csv")
                except Exception as e:
                    st.error(f"CSVå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

            # 2. çµæœãŒã‚°ãƒ©ãƒ• (Plotly Figure) ã®å ´åˆ
            elif isinstance(output, go.Figure):
                st.subheader("ç”Ÿæˆã•ã‚ŒãŸã‚°ãƒ©ãƒ•")
                st.plotly_chart(output, use_container_width=True)
                
                try:
                    img_bytes = output.to_image(format="png", scale=2)
                    st.download_button(
                        label="ã‚°ãƒ©ãƒ•ã‚’ç”»åƒ(PNG)ã§ä¿å­˜",
                        data=img_bytes,
                        file_name="chart.png",
                        mime="image/png"
                    )
                except Exception as e:
                    st.warning(f"ç”»åƒã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ (ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯å‹•ä½œã—ã¾ã™): {e}")

            # 3. çµæœãŒçµ±è¨ˆã‚µãƒãƒªãƒ¼ (æ–‡å­—åˆ—) ã®å ´åˆ
            elif isinstance(output, str):
                st.subheader("åˆ†æãƒ»æ¤œå®šçµæœ (ã‚µãƒãƒªãƒ¼)")
                st.text(output) 
            
            # 4. ãã®ä»–ã®çµæœ
            else:
                st.subheader("å®Ÿè¡Œçµæœ (ãã®ä»–)")
                st.write(output)
        else:
            st.info("Step 1ã§åˆ†ææŒ‡ç¤ºã‚’å‡ºã—ã€ã€Œåˆ†æã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

        # (UX CHANGE) å®Ÿè¡Œã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã¯ã€çµæœã®ä¸‹ã«æŠ˜ã‚ŠãŸãŸã‚“ã§è¡¨ç¤º
        if st.session_state.generated_code:
            with st.expander("ä»Šå›å®Ÿè¡Œã•ã‚ŒãŸPythonã‚³ãƒ¼ãƒ‰ã‚’è¡¨ç¤º"):
                st.code(st.session_state.generated_code, language="python")

    # --- å³ã‚«ãƒ©ãƒ  (col2): ãƒ‡ãƒ¼ã‚¿å‚ç…§ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ã‚¹ã‚­ãƒ¼ãƒï¼‰ ---
    with col2:
        st.header("ãƒ‡ãƒ¼ã‚¿å‚ç…§")
        
        st.subheader("ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­100è¡Œ (ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼)")
        st.dataframe(st.session_state.df.head(100), use_container_width=True, height=400)
        
        st.markdown("---")
        
        with st.expander("æ‹¡å¼µã‚¹ã‚­ãƒ¼ãƒ (AIã«é€ä¿¡ã™ã‚‹æƒ…å ±) ã‚’è¡¨ç¤º"):
            st.write("AIã¯ã“ã®ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ï¼ˆåˆ—åã€å‹ã€ã‚«ãƒ†ã‚´ãƒªã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ãªã©ï¼‰ã®ã¿ã‚’å‚ç…§ã—ã¦ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
            st.json(st.session_state.schema_dict, expanded=False)
